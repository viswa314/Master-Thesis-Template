
@article{jiang_review_2022,
	title = {A Review of Yolo Algorithm Developments},
	volume = {199},
	issn = {18770509},
	doi = {10.1016/j.procs.2022.01.135},
	pages = {1066--1073},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Jiang, Peiyuan and Ergu, Daji and Liu, Fangyao and Cai, Ying and Ma, Bo},
	date = {2022},
}

@article{lowe_distinctive_2004,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	pages = {91--110},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	urldate = {2024-06-19},
	date = {2004-11},
}

@inproceedings{rublee_orb_2011,
	title = {{ORB}: An efficient alternative to {SIFT} or {SURF}},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126544/},
	doi = {10.1109/ICCV.2011.6126544},
	shorttitle = {{ORB}},
	eventtitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2564--2571},
	booktitle = {2011 International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
	urldate = {2024-06-19},
	date = {2011-11},
	note = {Place: Barcelona, Spain},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of Oriented Gradients for Human Detection},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467360/},
	doi = {10.1109/CVPR.2005.177},
	eventtitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	pages = {886--893},
	booktitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	publisher = {{IEEE}},
	author = {Dalal, N. and Triggs, B.},
	urldate = {2024-06-19},
	date = {2005},
	note = {Place: San Diego, {CA}, {USA}},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/990517/},
	doi = {10.1109/CVPR.2001.990517},
	eventtitle = {2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {I--511--I--518},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	publisher = {{IEEE} Comput. Soc},
	author = {Viola, P. and Jones, M.},
	urldate = {2024-06-19},
	date = {2001},
	note = {Place: Kauai, {HI}, {USA}},
}

@incollection{bay_surf_2006,
	location = {Berlin, Heidelberg},
	title = {{SURF}: Speeded Up Robust Features},
	volume = {3951},
	isbn = {978-3-540-33832-1 978-3-540-33833-8},
	url = {http://link.springer.com/10.1007/11744023_32},
	shorttitle = {{SURF}},
	pages = {404--417},
	booktitle = {Computer Vision – {ECCV} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	urldate = {2024-06-19},
	date = {2006},
	doi = {10.1007/11744023_32},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2024-06-19},
	date = {2017-05-24},
}

@article{zou_object_2023,
	title = {Object Detection in 20 Years: A Survey},
	volume = {111},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/10028728/},
	doi = {10.1109/JPROC.2023.3238524},
	shorttitle = {Object Detection in 20 Years},
	pages = {257--276},
	number = {3},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2024-06-19},
	date = {2023-03},
}

@article{mur-artal_orb-slam_2015,
	title = {{ORB}-{SLAM}: A Versatile and Accurate Monocular {SLAM} System},
	volume = {31},
	issn = {1552-3098, 1941-0468},
	url = {https://ieeexplore.ieee.org/document/7219438/},
	doi = {10.1109/TRO.2015.2463671},
	shorttitle = {{ORB}-{SLAM}},
	pages = {1147--1163},
	number = {5},
	journaltitle = {{IEEE} Transactions on Robotics},
	shortjournal = {{IEEE} Trans. Robot.},
	author = {Mur-Artal, Raul and Montiel, J. M. M. and Tardos, Juan D.},
	urldate = {2024-06-19},
	date = {2015-10},
}

@incollection{shaikjee_analysis_2021,
	location = {Singapore},
	title = {Analysis of Traditional Computer Vision Techniques Used for Hemp Leaf Water Stress Detection and Classification},
	volume = {1184},
	isbn = {9789811558580 9789811558597},
	url = {http://link.springer.com/10.1007/978-981-15-5859-7_22},
	pages = {224--235},
	booktitle = {Proceedings of Fifth International Congress on Information and Communication Technology},
	publisher = {Springer Singapore},
	author = {Shaikjee, Waseem and Van Der Haar, Dustin},
	editor = {Yang, Xin-She and Sherratt, Simon and Dey, Nilanjan and Joshi, Amit},
	urldate = {2024-06-19},
	date = {2021},
	doi = {10.1007/978-981-15-5859-7_22},
}

@article{zhang_endoscope_2022,
	title = {Endoscope image mosaic based on pyramid {ORB}},
	volume = {71},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809421008582},
	doi = {10.1016/j.bspc.2021.103261},
	pages = {103261},
	journaltitle = {Biomedical Signal Processing and Control},
	shortjournal = {Biomedical Signal Processing and Control},
	author = {Zhang, Ziyan and Wang, Lixiao and Zheng, Wenfeng and Yin, Lirong and Hu, Rongrong and Yang, Bo},
	urldate = {2024-06-19},
	date = {2022-01},
}

@article{liu_orb-livox_2023,
	title = {{ORB}-Livox: A real-time dynamic system for fruit detection and localization},
	volume = {209},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169923002223},
	doi = {10.1016/j.compag.2023.107834},
	shorttitle = {{ORB}-Livox},
	pages = {107834},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Computers and Electronics in Agriculture},
	author = {Liu, Tianhao and Kang, Hanwen and Chen, Chao},
	urldate = {2024-06-19},
	date = {2023-06},
}

@inproceedings{das_vehicular_2023,
	title = {Vehicular Propagation Velocity Forecasting Using Open {CV}},
	isbn = {9798350393248},
	url = {https://ieeexplore.ieee.org/document/10449587/},
	doi = {10.1109/ICCAKM58659.2023.10449587},
	eventtitle = {2023 4th International Conference on Computation, Automation and Knowledge Management ({ICCAKM})},
	pages = {2--7},
	booktitle = {2023 4th International Conference on Computation, Automation and Knowledge Management ({ICCAKM})},
	publisher = {{IEEE}},
	author = {Das, Udayan and Sharma, Vandana and Das, Madhabananda and Mishra, Sushruta and Iwendi, Celestine and Osamor, Jude},
	urldate = {2024-06-19},
	date = {2023-12-12},
	note = {Place: Dubai, United Arab Emirates},
}

@inproceedings{collado_board_2007,
	title = {On Board Camera Perception and Tracking of Vehicles:},
	isbn = {978-972-8865-76-4},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0002066600570066},
	doi = {10.5220/0002066600570066},
	shorttitle = {On Board Camera Perception and Tracking of Vehicles},
	eventtitle = {International Workshop on Robot Vision},
	pages = {57--66},
	booktitle = {Robot Vision},
	publisher = {{SciTePress} - Science and and Technology Publications},
	author = {{Collado}},
	urldate = {2024-06-20},
	date = {2007},
	note = {Place: Barcelona, Spain},
}

@misc{marathe_rain_2022,
	title = {In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles},
	url = {https://arxiv.org/abs/2204.01062},
	shorttitle = {In Rain or Shine},
	abstract = {Several popular computer vision ({CV}) datasets, specifically employed for Object Detection ({OD}) in autonomous driving tasks exhibit biases due to a range of factors including weather and lighting conditions. These biases may impair a model's generalizability, rendering it ineffective for {OD} in novel and unseen datasets. Especially, in autonomous driving, it may prove extremely high risk and unsafe for the vehicle and its surroundings. This work focuses on understanding these datasets better by identifying such "good-weather" bias. Methods to mitigate such bias which allows the {OD} models to perform better and improve the robustness are also demonstrated. A simple yet effective {OD} framework for studying bias mitigation is proposed. Using this framework, the performance on popular datasets is analyzed and a significant difference in model performance is observed. Additionally, a knowledge transfer technique and a synthetic image corruption technique are proposed to mitigate the identified bias. Finally, using the {DAWN} dataset, the findings are validated on the {OD} task, demonstrating the effectiveness of our techniques in mitigating real-world "good-weather" bias. The experiments show that the proposed techniques outperform baseline methods by averaged fourfold improvement.},
	publisher = {{arXiv}},
	author = {Marathe, Aboli and Walambe, Rahee and Kotecha, Ketan},
	urldate = {2024-06-20},
	date = {2022},
	doi = {10.48550/ARXIV.2204.01062},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@inproceedings{chen_design_2021,
	title = {Design and Implementation of {AMR} Robot Based on {RGBD}, {VSLAM} and {SLAM}},
	isbn = {978-1-66547-842-7},
	url = {https://ieeexplore.ieee.org/document/9680621/},
	doi = {10.1109/ICOT54518.2021.9680621},
	eventtitle = {2021 9th International Conference on Orange Technology ({ICOT})},
	pages = {1--5},
	booktitle = {2021 9th International Conference on Orange Technology ({ICOT})},
	publisher = {{IEEE}},
	author = {Chen, Che-Wen and Lin, Chun-Liang and Hsu, Jia-Jiu and Tseng, Shih-Pang and Wang, Jhing-Fa},
	urldate = {2024-06-20},
	date = {2021-12-16},
	note = {Place: Tainan, Taiwan},
}

@inproceedings{ramesh_dfrc_2023,
	title = {{DFRC} Signal Classification: The Power of Deep Learning vs The Simplicity of Traditional Signal Processing},
	isbn = {9798350335095},
	url = {https://ieeexplore.ieee.org/document/10306697/},
	doi = {10.1109/ICCCNT56998.2023.10306697},
	shorttitle = {{DFRC} Signal Classification},
	eventtitle = {2023 14th International Conference on Computing Communication and Networking Technologies ({ICCCNT})},
	pages = {1--7},
	booktitle = {2023 14th International Conference on Computing Communication and Networking Technologies ({ICCCNT})},
	publisher = {{IEEE}},
	author = {Ramesh, Anvita and Adhikary, Aditi and {Ashwini} and Rai, Rishita and Samrin, Safa and Krishna A, Vijaya},
	urldate = {2024-06-20},
	date = {2023-07-06},
	note = {Place: Delhi, India},
}

@misc{aggrawal_state---art_2022,
	title = {State-of-the-art vs prominent models: An empirical analysis of various neural networks on stock market prediction},
	url = {https://engrxiv.org/preprint/view/2745/version/3944},
	shorttitle = {State-of-the-art vs prominent models},
	author = {Aggrawal, Tripti and Dhawan, Mansi},
	urldate = {2024-06-20},
	date = {2022-12-20},
	doi = {10.31224/2745},
}

@article{senior_protein_2019,
	title = {Protein structure prediction using multiple deep neural networks in the 13th Critical Assessment of Protein Structure Prediction ({CASP}13)},
	volume = {87},
	issn = {0887-3585, 1097-0134},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/prot.25834},
	doi = {10.1002/prot.25834},
	abstract = {Abstract We describe {AlphaFold}, the protein structure prediction system that was entered by the group A7D in {CASP}13. Submissions were made by three free‐modeling ({FM}) methods which combine the predictions of three neural networks. All three systems were guided by predictions of distances between pairs of residues produced by a neural network. Two systems assembled fragments produced by a generative neural network, one using scores from a network trained to regress {GDT}\_TS. The third system shows that simple gradient descent on a properly constructed potential is able to perform on par with more expensive traditional search techniques and without requiring domain segmentation. In the {CASP}13 {FM} assessors' ranking by summed z‐scores, this system scored highest with 68.3 vs 48.2 for the next closest group (an average {GDT}\_TS of 61.4). The system produced high‐accuracy structures (with {GDT}\_TS scores of 70 or higher) for 11 out of 43 {FM} domains. Despite not explicitly using template information, the results in the template category were comparable to the best performing template‐based methods.},
	pages = {1141--1148},
	number = {12},
	journaltitle = {Proteins: Structure, Function, and Bioinformatics},
	shortjournal = {Proteins},
	author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and Žídek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
	urldate = {2024-06-20},
	date = {2019-12},
}

@incollection{jiang_expert_2018,
	location = {Cham},
	title = {Expert Feature-Engineering vs. Deep Neural Networks: Which Is Better for Sensor-Free Affect Detection?},
	volume = {10947},
	isbn = {978-3-319-93842-4 978-3-319-93843-1},
	url = {http://link.springer.com/10.1007/978-3-319-93843-1_15},
	shorttitle = {Expert Feature-Engineering vs. Deep Neural Networks},
	pages = {198--211},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer International Publishing},
	author = {Jiang, Yang and Bosch, Nigel and Baker, Ryan S. and Paquette, Luc and Ocumpaugh, Jaclyn and Andres, Juliana Ma. Alexandra L. and Moore, Allison L. and Biswas, Gautam},
	editor = {Penstein Rosé, Carolyn and Martínez-Maldonado, Roberto and Hoppe, H. Ulrich and Luckin, Rose and Mavrikis, Manolis and Porayska-Pomsta, Kaska and {McLaren}, Bruce and Du Boulay, Benedict},
	urldate = {2024-06-20},
	date = {2018},
	doi = {10.1007/978-3-319-93843-1_15},
}

@inproceedings{house_evaluation_2017,
	title = {Evaluation of the Intel {RealSense} {SR}300 camera for image-guided interventions and application in vertebral level localization},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2255899},
	doi = {10.1117/12.2255899},
	eventtitle = {{SPIE} Medical Imaging},
	pages = {101352Z},
	author = {House, Rachael and Lasso, Andras and Harish, Vinyas and Baum, Zachary and Fichtinger, Gabor},
	editor = {Webster, Robert J. and Fei, Baowei},
	urldate = {2024-06-20},
	date = {2017-03-03},
	note = {Place: Orlando, Florida, United States},
}

@inproceedings{duraisamy_classroom_2019,
	title = {Classroom engagement evaluation using computer vision techniques},
	isbn = {978-1-5106-2655-3 978-1-5106-2656-0},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10995/2519266/Classroom-engagement-evaluation-using-computer-vision-techniques/10.1117/12.2519266.full},
	doi = {10.1117/12.2519266},
	eventtitle = {Pattern Recognition and Tracking {XXX}},
	pages = {31},
	booktitle = {Pattern Recognition and Tracking {XXX}},
	publisher = {{SPIE}},
	author = {Duraisamy, Prakash and Van Haneghan, James and Blackwell, William and Jackson, Stephen C. and G, Murugesan and K.S., Tamilselvan},
	editor = {Alam, Mohammad S.},
	urldate = {2024-06-20},
	date = {2019-05-29},
	note = {Place: Baltimore, United States},
}

@article{zahoor_breast_2021,
	title = {Breast Cancer Detection and Classification using Traditional Computer Vision Techniques: A Comprehensive Review},
	volume = {16},
	issn = {15734056},
	url = {https://www.eurekaselect.com/180730/article},
	doi = {10.2174/1573405616666200406110547},
	shorttitle = {Breast Cancer Detection and Classification using Traditional Computer Vision Techniques},
	abstract = {Breast Cancer is a common dangerous disease for women. Around the world, many women have died due to Breast cancer. However, in the initial stage, the diagnosis of breast cancer can save women\&\#039;s life. To diagnose cancer in the breast tissues, there are several techniques and methods. The image processing, machine learning, and deep learning methods and techniques are presented in this paper to diagnose the breast cancer. This work will be helpful to adopt better choices and reliable methods to diagnose breast cancer in an initial stage to save a women\&\#039;s life. To detect the breast masses, microcalcifications, and malignant cells,different techniques are used in the Computer-Aided Diagnosis ({CAD}) systems phases like preprocessing, segmentation, feature extraction, and classification. We have reported a detailed analysis of different techniques or methods with their usage and performance measurement. From the reported results, it is concluded that for breast cancer survival, it is essential to improve the methods or techniques to diagnose it at an initial stage by improving the results of the Computer-Aided Diagnosis systems. Furthermore, segmentation and classification phases are also challenging for researchers for the diagnosis of breast cancer accurately. Therefore, more advanced tools and techniques are still essential for the accurate diagnosis and classification of breast cancer.},
	pages = {1187--1200},
	number = {10},
	journaltitle = {Current Medical Imaging Formerly Current Medical Imaging Reviews},
	shortjournal = {{CMIR}},
	author = {Zahoor, Saliha and Lali, Ikram Ullah and Khan, Muhammad Attique and Javed, Kashif and Mehmood, Waqar},
	urldate = {2024-06-20},
	date = {2021-01-12},
}

@article{hubner_evaluation_2023,
	title = {Evaluation of Intel {RealSense} D455 Camera Depth Estimation for Indoor {SLAM} Applications},
	volume = {{XLVIII}-1/W2-2023},
	issn = {2194-9034},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-1-W2-2023/1207/2023/},
	doi = {10.5194/isprs-archives-XLVIII-1-W2-2023-1207-2023},
	abstract = {Abstract. The aim of this study is to propose evaluation methodology for the quality assessment of depth cameras for indoor mapping applications. Specifically, we evaluate the {RGBD} sensor Intel Realsense D455 w.r.t. measurement accuracy and noise while investigating their dependence on two central parameters characterizing the measurement scenario simultaneously: measurement distance and inclination of observed surfaces. The evaluation results are presented two-dimensionally as a function of both parameters. To this aim, two evaluation studies are conducted. First, a checkerboard pattern is used as reference object in a controlled setting. Secondly, actual indoor mapping data resulting from the deployment of the evaluated sensor in the context of an {RGBD}-{SLAM} application is used to conduct a similar evaluation in representative conditions for the intended usage scenario of indoor mapping. In this case, a {TLS} scan of the respective indoor environment is used as reference for evaluation.},
	pages = {1207--1214},
	journaltitle = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Hübner, P. and Hou, J. and Iwaszczuk, D.},
	urldate = {2024-06-20},
	date = {2023-12-13},
}

@article{mahony_deep_2019,
	title = {Deep Learning vs. Traditional Computer Vision},
	url = {https://arxiv.org/abs/1910.13796},
	doi = {10.48550/ARXIV.1910.13796},
	abstract = {Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of {DL} have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised},
	author = {Mahony, Niall O' and Campbell, Sean and Carvalho, Anderson and Harapanahalli, Suman and Velasco-Hernandez, Gustavo and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph},
	urldate = {2024-06-20},
	date = {2019},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@misc{bochkovskiy_yolov4_2020,
	title = {{YOLOv}4: Optimal Speed and Accuracy of Object Detection},
	url = {https://arxiv.org/abs/2004.10934},
	shorttitle = {{YOLOv}4},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network ({CNN}) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections ({WRC}), Cross-Stage-Partial-connections ({CSP}), Cross mini-Batch Normalization ({CmBN}), Self-adversarial-training ({SAT}) and Mish-activation. We use new features: {WRC}, {CSP}, {CmBN}, {SAT}, Mish activation, Mosaic data augmentation, {CmBN}, {DropBlock} regularization, and {CIoU} loss, and combine some of them to achieve state-of-the-art results: 43.5\% {AP} (65.7\% {AP}50) for the {MS} {COCO} dataset at a realtime speed of {\textbackslash}textasciitilde65 {FPS} on Tesla V100. Source code is at https://github.com/{AlexeyAB}/darknet},
	publisher = {{arXiv}},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	urldate = {2024-06-21},
	date = {2020},
	doi = {10.48550/ARXIV.2004.10934},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, electronic engineering, {FOS}: Electrical engineering, Image and Video Processing (eess.{IV}), information engineering},
}

@misc{redmon_you_2015,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {https://arxiv.org/abs/1506.02640},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base {YOLO} model processes images in real-time at 45 frames per second. A smaller version of the network, Fast {YOLO}, processes an astounding 155 frames per second while still achieving double the {mAP} of other real-time detectors. Compared to state-of-the-art detection systems, {YOLO} makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, {YOLO} learns very general representations of objects. It outperforms all other detection methods, including {DPM} and R-{CNN}, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	publisher = {{arXiv}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2024-06-21},
	date = {2015},
	doi = {10.48550/ARXIV.1506.02640},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{redmon_yolo9000_2016,
	title = {{YOLO}9000: Better, Faster, Stronger},
	url = {https://arxiv.org/abs/1612.08242},
	shorttitle = {{YOLO}9000},
	abstract = {We introduce {YOLO}9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the {YOLO} detection method, both novel and drawn from prior work. The improved model, {YOLOv}2, is state-of-the-art on standard detection tasks like {PASCAL} {VOC} and {COCO}. At 67 {FPS}, {YOLOv}2 gets 76.8 {mAP} on {VOC} 2007. At 40 {FPS}, {YOLOv}2 gets 78.6 {mAP}, outperforming state-of-the-art methods like Faster {RCNN} with {ResNet} and {SSD} while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train {YOLO}9000 simultaneously on the {COCO} detection dataset and the {ImageNet} classification dataset. Our joint training allows {YOLO}9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the {ImageNet} detection task. {YOLO}9000 gets 19.7 {mAP} on the {ImageNet} detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in {COCO}, {YOLO}9000 gets 16.0 {mAP}. But {YOLO} can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	publisher = {{arXiv}},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2024-06-21},
	date = {2016},
	doi = {10.48550/ARXIV.1612.08242},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{lou_dc-yolov8_2023,
	title = {{DC}-{YOLOv}8: Small Size Object Detection Algorithm Based on Camera Sensor},
	url = {https://www.preprints.org/manuscript/202304.0124/v1},
	shorttitle = {{DC}-{YOLOv}8},
	abstract = {Traditional camera sensors rely on human eyes for observation. However, the human eye 1 is prone to fatigue when observing targets of different sizes for a long time in complex scenes, and 2 human cognition is limited, which often leads to judgment errors and greatly reduces the efficiency. 3 Target recognition technology is an important technology to judge the target category in camera 4 sensor. In order to solve this problem, a small size target detection algorithm for special scenarios was 5 proposed by this paper. Its advantage is that this algorithm not only has higher precision for small 6 size target detection, but also can ensure that the detection accuracy of each size is not lower than the 7 existing algorithm. In this paper, a new down-sampling method was proposed, which could better 8 preserve the context feature information. The feature fusion network was improved to effectively 9 combine shallow information and deep information. A new network structure was proposed to 10 effectively improve the detection accuracy of the model. In terms of accuracy, it is better than: {YOLOX}, 11 {YOLOXR}, {YOLOv}3, scaled {YOLOv}5, {YOLOv}7-Tiny and {YOLOv}8.Three authoritative public data sets 12 were used in this experiment: a) On Visdron data sets (small size targets), {DC}-{YOLOv}8 is 2.5\% more 13 accurate than {YOLOv}8. b) On Tinyperson data sets (minimal size targets), {DC}-{YOLOv}8 is 1\% more 14 accurate than {YOLOv}8. c) On {PASCAL} {VOC}2007 data sets (Normal size target), {DC}-{YOLOv}8 is 0.5\% 15 more accurate than {YOLOv}8.},
	author = {Lou, Haitong and Duan, Xuehu and Guo, Junmei and Liu, Haiying and Gu, Jason and Bi, Lingyun and Chen, Haonan},
	urldate = {2024-06-21},
	date = {2023-04-07},
	doi = {10.20944/preprints202304.0124.v1},
}

@misc{reis_real-time_2023,
	title = {Real-Time Flying Object Detection with {YOLOv}8},
	url = {https://arxiv.org/abs/2305.09972},
	abstract = {This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that achieves state-of-the-art results for flying object detection. We achieve this by training our first (generalized) model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of real world environments (i.e. higher frequency of occlusion, very small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variances of object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state-of-the-art single-shot detector, {YOLOv}8, in an attempt to find the best trade-off between inference speed and mean average precision ({mAP}). While {YOLOv}8 is being regarded as the new state-of-the-art, an official paper has not been released as of yet. Thus, we provide an in-depth explanation of the new architecture and functionality that {YOLOv}8 has adapted. Our final generalized model achieves a {mAP}50 of 79.2\%, {mAP}50-95 of 68.5\%, and an average inference speed of 50 frames per second (fps) on 1080p videos. Our final refined model maintains this inference speed and achieves an improved {mAP}50 of 99.1\% and {mAP}50-95 of 83.5\%},
	publisher = {{arXiv}},
	author = {Reis, Dillon and Kupec, Jordan and Hong, Jacqueline and Daoudi, Ahmad},
	urldate = {2024-06-21},
	date = {2023},
	doi = {10.48550/ARXIV.2305.09972},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), I.2.10, I.2.6},
}

@misc{zhao_detrs_2023,
	title = {{DETRs} Beat {YOLOs} on Real-time Object Detection},
	url = {https://arxiv.org/abs/2304.08069},
	abstract = {The {YOLO} series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of {YOLOs} are negatively affected by the {NMS}. Recently, end-to-end Transformer-based detectors ({DETRs}) have provided an alternative to eliminating {NMS}. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding {NMS}. In this paper, we propose the Real-Time {DEtection} {TRansformer} ({RT}-{DETR}), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build {RT}-{DETR} in two steps, drawing on the advanced {DETR}: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, {RT}-{DETR} supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our {RT}-{DETR}-R50 / R101 achieves 53.1\% / 54.3\% {AP} on {COCO} and 108 / 74 {FPS} on T4 {GPU}, outperforming previously advanced {YOLOs} in both speed and accuracy. We also develop scaled {RT}-{DETRs} that outperform the lighter {YOLO} detectors (S and M models). Furthermore, {RT}-{DETR}-R50 outperforms {DINO}-R50 by 2.2\% {AP} in accuracy and about 21 times in {FPS}. After pre-training with Objects365, {RT}-{DETR}-R50 / R101 achieves 55.3\% / 56.2\% {AP}. The project page: https://zhao-yian.github.io/{RTDETR}.},
	publisher = {{arXiv}},
	author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
	urldate = {2024-06-21},
	date = {2023},
	doi = {10.48550/ARXIV.2304.08069},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{zhao_real-time_2022,
	title = {Real-time Online Video Detection with Temporal Smoothing Transformers},
	url = {https://arxiv.org/abs/2209.09236},
	abstract = {Streaming video recognition reasons about objects and their actions in every frame of a video. A good streaming recognition model captures both long-term dynamics and short-term changes of video. Unfortunately, in most existing methods, the computational complexity grows linearly or quadratically with the length of the considered dynamics. This issue is particularly pronounced in transformer-based architectures. To address this issue, we reformulate the cross-attention in a video transformer through the lens of kernel and apply two kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The resulting streaming attention reuses much of the computation from frame to frame, and only requires a constant time update each frame. Based on this idea, we build {TeSTra}, a Temporal Smoothing Transformer, that takes in arbitrarily long inputs with constant caching and computing overhead. Specifically, it runs {\textbackslash}6{\textbackslash}textbackslashtimes{\textbackslash} faster than equivalent sliding-window based transformers with 2,048 frames in a streaming setting. Furthermore, thanks to the increased temporal span, {TeSTra} achieves state-of-the-art results on {THUMOS}'14 and {EPIC}-Kitchen-100, two standard online action detection and action anticipation datasets. A real-time version of {TeSTra} outperforms all but one prior approaches on the {THUMOS}'14 dataset.},
	publisher = {{arXiv}},
	author = {Zhao, Yue and Krähenbühl, Philipp},
	urldate = {2024-06-21},
	date = {2022},
	doi = {10.48550/ARXIV.2209.09236},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@article{li_fish_2023,
	title = {Fish Detection under Occlusion Using Modified You Only Look Once v8 Integrating Real-Time Detection Transformer Features},
	volume = {13},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/23/12645},
	doi = {10.3390/app132312645},
	abstract = {Fish object detection has attracted significant attention because of the considerable role that fish play in human society and ecosystems and the necessity to gather more comprehensive fish data through underwater videos or images. However, fish detection has always faced difficulties with the occlusion problem because of dense populations and underwater plants that obscure them, and no perfect solution has been found until now. To address the occlusion issue in fish detection, the following effort was made: creating a dataset of occluded fishes, integrating the innovative modules in Real-time Detection Transformer ({RT}-{DETR}) into You Only Look Once v8 ({YOLOv}8), and applying repulsion loss. The results show that in the occlusion dataset, the {mAP} of the original {YOLOv}8 is 0.912, while the {mAP} of our modified {YOLOv}8 is 0.971. In addition, our modified {YOLOv}8 also has better performance than the original {YOLOv}8 in terms of loss curves, F1–Confidence curves, P–R curves, the {mAP} curve and the actual detection effects. All these indicate that our modified {YOLOv}8 is suitable for fish detection in occlusion scenes.},
	pages = {12645},
	number = {23},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Li, Enze and Wang, Qibiao and Zhang, Jinzhao and Zhang, Weihan and Mo, Hanlin and Wu, Yadong},
	urldate = {2024-06-21},
	date = {2023-11-24},
}

@misc{chen_real-time_2023,
	title = {Real-time Network Intrusion Detection via Decision Transformers},
	url = {https://arxiv.org/abs/2312.07696},
	abstract = {Many cybersecurity problems that require real-time decision-making based on temporal observations can be abstracted as a sequence modeling problem, e.g., network intrusion detection from a sequence of arriving packets. Existing approaches like reinforcement learning may not be suitable for such cybersecurity decision problems, since the Markovian property may not necessarily hold and the underlying network states are often not observable. In this paper, we cast the problem of real-time network intrusion detection as casual sequence modeling and draw upon the power of the transformer architecture for real-time decision-making. By conditioning a causal decision transformer on past trajectories, consisting of the rewards, network packets, and detection decisions, our proposed framework will generate future detection decisions to achieve the desired return. It enables decision transformers to be applied to real-time network intrusion detection, as well as a novel tradeoff between the accuracy and timeliness of detection. The proposed solution is evaluated on public network intrusion detection datasets and outperforms several baseline algorithms using reinforcement learning and sequence modeling, in terms of detection accuracy and timeliness.},
	publisher = {{arXiv}},
	author = {Chen, Jingdi and Zhou, Hanhan and Mei, Yongsheng and Adam, Gina and Bastian, Nathaniel D. and Lan, Tian},
	urldate = {2024-06-21},
	date = {2023},
	doi = {10.48550/ARXIV.2312.07696},
	keywords = {{FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Cryptography and Security (cs.{CR})},
}

@inproceedings{gupta_haarcascade_2023,
	title = {{HaarCascade} and {LBPH} Algorithms in Face Recognition Analysis},
	isbn = {9798350311204},
	url = {https://ieeexplore.ieee.org/document/10235019/},
	doi = {10.1109/WCONF58270.2023.10235019},
	eventtitle = {2023 World Conference on Communication \& Computing ({WCONF})},
	pages = {1--4},
	booktitle = {2023 World Conference on Communication \& Computing ({WCONF})},
	publisher = {{IEEE}},
	author = {Gupta, Mridul and Bisht, Karan and Sharma, Abhay and Upadhyay, Deepak},
	urldate = {2024-06-21},
	date = {2023-07-14},
	note = {Place: {RAIPUR}, India},
}

@article{ortatas_solution_2023,
	title = {Solution of Real-Time Traffic Signs Detection Problem for Autonomous Vehicles by Using {YOLOV}4 And Haarcascade Algorithms},
	volume = {7},
	issn = {2587-0963},
	url = {http://dergipark.org.tr/en/doi/10.30939/ijastech..1231646},
	doi = {10.30939/ijastech..1231646},
	abstract = {Unmanned systems are increasingly used today to facilitate our daily lives and use time more efficiently. Therefore, this rapidly emerging and growing technology appears in every aspect of our lives with its various functions. Object recognition algorithms are one of the most important functions that we often encounter in these systems. Autonomous vehicle technologies are the latest and fastest growing technology among unmanned systems. In this study, we investigate the success rates of two different algorithms for recognizing traffic signs and markings that can be used for partially or fully autonomous vehicles. In this study, two different solutions to the problem of recognizing the signs for fully autonomous and fully autonomous vehicles, respectively, were presented and the correct identification of the markers was evaluated. The work was performed in real-time. Two different concepts were used for these products. An enclosed space where an ideal lighting environment is provided for the evaluation of models should be visualized. In addition, for the general recognition of the models, the test procedures were performed with a dataset obtained from the users and it was computed for the general recognition. In addition, this study aims to provide a better understanding of the basic working principles, the differences between machine learning and deep learning, and the contents of object recognition processes.},
	pages = {125--140},
	number = {2},
	journaltitle = {International Journal of Automotive Science and Technology},
	author = {Ortataş, Fatma Nur and Çeti̇N, Emrah},
	urldate = {2024-06-21},
	date = {2023-06-30},
}

@article{gunawan_indonesia_2023,
	title = {Indonesia Rancang bangun aplikasi absensi mahasiswa menggunakan metode Viola Jhones algoritma {HaarCascade} di {UIM}},
	volume = {13},
	issn = {2962-2565, 2088-4591},
	url = {https://ejournal.upm.ac.id/index.php/energy/article/view/1052},
	doi = {10.51747/energy.v13i1.1052},
	abstract = {Pengenalan citra wajah manusia merupakan salah satu teknologi utama yang terus dikembangkan. Di bidang Computer Vision dalam penerapannya dalam sistem pengenalan biomatrik, Sistem pencarian, pengindeksan pada database video digital, sistem keamanan kontrol akses area terbatas, konferensi video, interaksi manusia dengan komputer. dan lain sebagainya. Metode Viola-Jones adalah metode deteksi objek yang memiliki akurasi yang cukup tinggi yaitu sekitar 93,7\% dengan kecepatan 15 kali lebih cepat dari detektor Rowley Baluja-Kanade dan sekitar 600 kali lebih cepat dari detektor Schneiderman-Kanade. Algoritma Haar Cascade Classifier adalah salah satu algoritma yang digunakan untuk mendeteksi sebuah wajah. Cascade Classifier digunakan dalam mendata absensi dengan pengenalan wajah yang dapat mendata mahasiswa secara real-time . Kata Kunci: Deteksi wajah, {OpenCV}, Haar Cascade},
	pages = {8--15},
	number = {1},
	journaltitle = {Energy - Jurnal Ilmiah Ilmu-Ilmu Teknik},
	shortjournal = {energy},
	author = {Gunawan, Lukman Syahrul and Umam, Busro Akramul and Makruf, Masdukil},
	urldate = {2024-06-21},
	date = {2023-07-20},
}

@inproceedings{rubiston_vehicle_2023,
	title = {Vehicle Speed Determination Using Haarcascade Algorithm},
	isbn = {9798350384208},
	url = {https://ieeexplore.ieee.org/document/10407050/},
	doi = {10.1109/ICSSS58085.2023.10407050},
	eventtitle = {2023 9th International Conference on Smart Structures and Systems ({ICSSS})},
	pages = {1--5},
	booktitle = {2023 9th International Conference on Smart Structures and Systems ({ICSSS})},
	publisher = {{IEEE}},
	author = {Rubiston, M. Maria and K, Harigaran and S, Darshini and B, Mukeshkumar and U, Santhosh and J, Albee Micheal},
	urldate = {2024-06-21},
	date = {2023-11-23},
	note = {Place: {CHENNAI}, India},
}

@article{sethy_detection_2020,
	title = {Detection of coronavirus Disease ({COVID}-19) based on Deep Features and Support Vector Machine},
	volume = {5},
	issn = {2455-7749},
	url = {https://www.ijmems.in/volumes/volume5/number4/52-IJMEMS-20-46-54-643-651-2020.pdf},
	doi = {10.33889/IJMEMS.2020.5.4.052},
	abstract = {The detection of coronavirus ({COVID}-19) is now a critical task for the medical practitioner. The coronavirus spread so quickly between people and approaches 100,000 people worldwide. In this consequence, it is very much essential to identify the infected people so that prevention of spread can be taken. In this paper, the deep feature plus support vector machine ({SVM}) based methodology is suggested for detection of coronavirus infected patient using X-ray images. For classification, {SVM} is used instead of deep learning based classifier, as the later one need a large dataset for training and validation. The deep features from the fully connected layer of {CNN} model are extracted and fed to {SVM} for classification purpose. The {SVM} classifies the corona affected X-ray images from others. The methodology consists of three categories of Xray images, i.e., {COVID}-19, pneumonia and normal. The method is beneficial for the medical practitioner to classify among the {COVID}-19 patient, pneumonia patient and healthy people. {SVM} is evaluated for detection of {COVID}-19 using the deep features of different 13 number of {CNN} models. The {SVM} produced the best results using the deep feature of {ResNet}50. The classification model, i.e. {ResNet}50 plus {SVM} achieved accuracy, sensitivity, {FPR} and F1 score of 95.33\%,95.33\%,2.33\% and 95.34\% respectively for detection of {COVID}-19 (ignoring {SARS}, {MERS} and {ARDS}). Again, the highest accuracy achieved by {ResNet}50 plus {SVM} is 98.66\%. The result is based on the Xray images available in the repository of {GitHub} and Kaggle. As the data set is in hundreds, the classification based on {SVM} is more robust compared to the transfer learning approach. Also, a comparison analysis of other traditional classification method is carried out. The traditional methods are local binary patterns ({LBP}) plus {SVM}, histogram of oriented gradients ({HOG}) plus {SVM} and Gray Level Co-occurrence Matrix ({GLCM}) plus {SVM}. In traditional image classification method, {LBP} plus {SVM} achieved 93.4\% of accuracy.},
	pages = {643--651},
	number = {4},
	journaltitle = {International Journal of Mathematical, Engineering and Management Sciences},
	shortjournal = {Int J Math, Eng, Manag Sci},
	author = {Sethy, Prabira Kumar and Behera, Santi Kumari and Ratha, Pradyumna Kumar and Biswas, Preesat},
	urldate = {2024-06-21},
	date = {2020-08-01},
}

@inproceedings{meus_embedded_2017,
	location = {Poznan},
	title = {Embedded vision system for pedestrian detection based on {HOG}+{SVM} and use of motion information implemented in Zynq heterogeneous device},
	isbn = {978-83-62065-30-1},
	url = {http://ieeexplore.ieee.org/document/8166901/},
	doi = {10.23919/SPA.2017.8166901},
	eventtitle = {2017 Signal Processing: Algorithms, Architectures, Arrangements and Applications ({SPA})},
	pages = {406--411},
	booktitle = {2017 Signal Processing: Algorithms, Architectures, Arrangements, and Applications ({SPA})},
	publisher = {{IEEE}},
	author = {Meus, Bartosz and Kryjak, Tomasz and Gorgon, Marek},
	urldate = {2024-06-24},
	date = {2017-09},
}

@inproceedings{nguyen_novel_2019,
	location = {Bangkok, Thailand},
	title = {A Novel Hardware Architecture for Human Detection using {HOG}-{SVM} Co-Optimization},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72812-940-2},
	url = {https://ieeexplore.ieee.org/document/8953123/},
	doi = {10.1109/APCCAS47518.2019.8953123},
	eventtitle = {2019 {IEEE} Asia Pacific Conference on Circuits and Systems ({APCCAS})},
	pages = {33--36},
	booktitle = {2019 {IEEE} Asia Pacific Conference on Circuits and Systems ({APCCAS})},
	publisher = {{IEEE}},
	author = {Nguyen, Ngo-Doanh and Bui, Duy-Hieu and Tran, Xuan-Tu},
	urldate = {2024-06-24},
	date = {2019-11},
}

@article{rao_face_nodate,
	title = {Face Detection and Tracking to Find Missing Person Using Haarcascade Algorithm},
	volume = {8},
	url = {https://ijrti.org/papers/IJRTI2307048.pdf},
	number = {7},
	journaltitle = {Conference Proceedings},
	author = {Rao, Dr.{GOLAGANI}.A.V.R.C and B, Jayasri and G, Sai Sruthi and B, Rohit and D, Gowtham Sampath Sai},
}

@article{rubin_attention_nodate,
	title = {Attention Distillation for Detection Transformers: Application to Real-Time Video Object Detection in Ultrasound},
	url = {https://proceedings.mlr.press/v158/rubin21a/rubin21a.pdf},
	author = {Rubin, Jonathan and Erkamp, Ramon and Naidu, Ragha Srinivas and Anumod, Odungatta Thodiyil and Alvin, Chen},
}

@article{huang_machine_2022,
	title = {Machine learning in sustainable ship design and operation: A review},
	volume = {266},
	issn = {00298018},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0029801822021904},
	doi = {10.1016/j.oceaneng.2022.112907},
	shorttitle = {Machine learning in sustainable ship design and operation},
	pages = {112907},
	journaltitle = {Ocean Engineering},
	shortjournal = {Ocean Engineering},
	author = {Huang, Luofeng and Pena, Blanca and Liu, Yuanchang and Anderlini, Enrico},
	urldate = {2024-06-24},
	date = {2022-12},
	langid = {english},
	file = {Full Text:C\:\\Users\\gvisw\\Zotero\\storage\\DQ6TB2LZ\\Huang et al. - 2022 - Machine learning in sustainable ship design and op.pdf:application/pdf},
}

@article{singh_feature_2024,
	title = {Feature Engineering for Images: A Valuable Introduction to the {HOG} Feature Descriptor},
	url = {https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/},
	author = {Singh, Aishwarya},
	date = {2024-02-26},
}

@article{sciki-learn_support_nodate,
	title = {Support Vector Machines},
	url = {https://scikit-learn.org/stable/modules/svm.html},
	author = {Sciki-learn},
}

@article{chhajro_pedestrian_2018,
	title = {Pedestrian Detection Approach for Driver Assisted System using Haar based Cascade Classifiers},
	volume = {9},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=9&Issue=6&Code=ijacsa&SerialNo=16},
	doi = {10.14569/IJACSA.2018.090616},
	number = {6},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	shortjournal = {ijacsa},
	author = {Chhajro, M. Ameen and Kumar, Kamlesh and Malook, M. and Ahmed, Aftab and Nawaz, Haque and Hussain, Rafaqat},
	urldate = {2024-06-24},
	date = {2018},
	langid = {english},
	file = {Full Text:C\:\\Users\\gvisw\\Zotero\\storage\\SZYIF678\\Chhajro et al. - 2018 - Pedestrian Detection Approach for Driver Assisted .pdf:application/pdf},
}

@book{klette_concise_2014,
	location = {London},
	title = {Concise Computer Vision: An Introduction into Theory and Algorithms},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4471-6319-0 978-1-4471-6320-6},
	url = {https://link.springer.com/10.1007/978-1-4471-6320-6},
	series = {Undergraduate Topics in Computer Science},
	shorttitle = {Concise Computer Vision},
	publisher = {Springer London},
	author = {Klette, Reinhard},
	urldate = {2024-06-24},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-1-4471-6320-6},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\9TR7R8Y6\\Klette - 2014 - Concise Computer Vision An Introduction into Theo.pdf:application/pdf},
}

@incollection{ranganathan_human_2022,
	location = {Singapore},
	title = {Human Face Recognition Applying Haar Cascade Classifier},
	volume = {317},
	isbn = {9789811656392 9789811656408},
	url = {https://link.springer.com/10.1007/978-981-16-5640-8_12},
	pages = {143--157},
	booktitle = {Pervasive Computing and Social Networking},
	publisher = {Springer Nature Singapore},
	author = {Javed Mehedi Shamrat, F. M. and Majumder, Anup and Antu, Probal Roy and Barmon, Saykot Kumar and Nowrin, Itisha and Ranjan, Rumesh},
	editor = {Ranganathan, G. and Bestak, Robert and Palanisamy, Ram and Rocha, Álvaro},
	urldate = {2024-06-24},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-981-16-5640-8_12},
	note = {Series Title: Lecture Notes in Networks and Systems},
}

@inproceedings{lienhart_extended_2002,
	location = {Rochester, {NY}, {USA}},
	title = {An extended set of Haar-like features for rapid object detection},
	volume = {1},
	isbn = {978-0-7803-7622-9},
	url = {http://ieeexplore.ieee.org/document/1038171/},
	doi = {10.1109/ICIP.2002.1038171},
	eventtitle = {{ICIP} 2002 International Conference on Image Processing},
	pages = {I--900--I--903},
	booktitle = {Proceedings. International Conference on Image Processing},
	publisher = {{IEEE}},
	author = {Lienhart, R. and Maydt, J.},
	urldate = {2024-06-24},
	date = {2002},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\Q2ZMEFJ9\\Lienhart and Maydt - 2002 - An extended set of Haar-like features for rapid ob.pdf:application/pdf},
}

@article{nelson_integral_2017,
	title = {Integral Image in Hardware: Convolutional Summed Area Tables for Fast Features Computation},
	url = {https://sistenix.com/integral.html},
	author = {Nelson, Campos},
	date = {2017-04-02},
}

@misc{wikipedia_contributors_summed-area_2024,
	title = {Summed-area table — Wikipedia, The Free Encyclopedia},
	url = {https://en.wikipedia.org/w/index.php?title=Summed-area_table&oldid=1221815425},
	author = {{Wikipedia contributors}},
	date = {2024},
}

@article{rahmad_comparison_2020,
	title = {Comparison of Viola-Jones Haar Cascade Classifier and Histogram of Oriented Gradients ({HOG}) for face detection},
	volume = {732},
	issn = {1757-8981, 1757-899X},
	url = {https://iopscience.iop.org/article/10.1088/1757-899X/732/1/012038},
	doi = {10.1088/1757-899X/732/1/012038},
	abstract = {Abstract
            Human face recognition is one of the most challenging topics in the areas of image processing, computer vision, and pattern recognition. Before recognizing the human face, it is necessary to detect a face then extract the face features. Many methods have been created and developed in order to perform face detection and two of the most popular methods are Viola-Jones Haar Cascade Classifier (V-J) and Histogram of Oriented Gradients ({HOG}). This paper proposed a comparison between {VJ} and {HOG} for detecting the face. V-J method calculate Integral Image through Haar-like feature with {AdaBoost} process to make a robust cascade classifier, {HOG} compute the classifier for each image in and scale of the image, applied the sliding windows, extracted {HOG} descriptor at each window and applied the classifier, if the classifier detected an object with enough probability that resembles a face, the classifier recording the bounding box of the window and applied non-maximum suppression to make the accuracy increased. The experimental results show that the system successfully detected face based on the determined algorithm. That is mean the application using computer vision can detect face and compare the results.},
	pages = {012038},
	number = {1},
	journaltitle = {{IOP} Conference Series: Materials Science and Engineering},
	shortjournal = {{IOP} Conf. Ser.: Mater. Sci. Eng.},
	author = {Rahmad, C and Asmara, R A and Putra, D R H and Dharma, I and Darmono, H and Muhiqqin, I},
	urldate = {2024-06-24},
	date = {2020-01-01},
}

@article{everingham_pascal_2010,
	title = {The Pascal Visual Object Classes ({VOC}) Challenge},
	volume = {88},
	rights = {http://www.springer.com/tdm},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	pages = {303--338},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	urldate = {2024-06-24},
	date = {2010-06},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\MLIPLA95\\Everingham et al. - 2010 - The Pascal Visual Object Classes (VOC) Challenge.pdf:application/pdf},
}

@article{hosang_what_2016,
	title = {What Makes for Effective Detection Proposals?},
	volume = {38},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7182356/},
	doi = {10.1109/TPAMI.2015.2465908},
	pages = {814--830},
	number = {4},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Hosang, Jan and Benenson, Rodrigo and Dollar, Piotr and Schiele, Bernt},
	urldate = {2024-06-24},
	date = {2016-04-01},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\IZXCB896\\Hosang et al. - 2016 - What Makes for Effective Detection Proposals.pdf:application/pdf},
}

@incollection{jacob_review_2024,
	location = {Singapore},
	title = {A Review on {YOLOv}8 and Its Advancements},
	isbn = {978-981-9979-99-8 978-981-9979-62-2},
	url = {https://link.springer.com/10.1007/978-981-99-7962-2_39},
	pages = {529--545},
	booktitle = {Data Intelligence and Cognitive Informatics},
	publisher = {Springer Nature Singapore},
	author = {Sohan, Mupparaju and Sai Ram, Thotakura and Rami Reddy, Ch. Venkata},
	editor = {Jacob, I. Jeena and Piramuthu, Selwyn and Falkowski-Gilski, Przemyslaw},
	urldate = {2024-06-24},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-981-99-7962-2_39},
	note = {Series Title: Algorithms for Intelligent Systems},
}

@misc{li_research_2024,
	title = {Research on {YOLOv}8 Object Detection Algorithm in {UAV} Scenarios},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	url = {https://www.researchsquare.com/article/rs-3995816/v1},
	doi = {10.21203/rs.3.rs-3995816/v1},
	abstract = {Abstract
          Aerial images captured by Unmanned Aerial Vehicles ({UAVs}) often exhibit characteristics such as high object density, small targets, and wide coverage, which lead to the occurrence of false positives and false negatives in existing object detectors. In order to improve detection accuracy, this study proposes an enhanced {YOLOv}8 object detection model. Firstly, the Deformable Convolutional Networks v2 ({DConv}2) is incorporated into the backbone's C2F module to expand the receptive field of the image and enhance the detection accuracy of small objects. Secondly, the Fousion Block module is employed in the neck network to increase network depth and strengthen feature fusion capability. Subsequently, the model incorporates up sampling operations and discards large object detection layers, further improving the detection progress for small objects. Finally, the Efficient Intersection over Union ({ECIoU}) loss function is adopted to replace the Complete Intersection over Union ({CIoU}) loss function, accelerating the model's convergence speed and enhancing object detection precision. The improved model is validated using the {VisDrone}2019-{DET}-train dataset, and the results demonstrate that the enhanced model Yolov8s\_UAU achieves {anmAP} of 49.5\%, representing a 7.3\% improvement over traditional models. With the capability to maintain {UAV} detection speed, the proposed model can more accurately accomplish the detection tasks for small targets during {UAV} aerial photography processes.},
	author = {Li, Shixin and Chen, Fankai and Sun, Zhongyuan and Zhu, Zhiren and Zhou, Liming and Tang, Kaiwen},
	urldate = {2024-06-24},
	date = {2024-03-01},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\QMVQE5G6\\Li et al. - 2024 - Research on YOLOv8 Object Detection Algorithm in U.pdf:application/pdf},
}

@article{smith_susannew_1997,
	title = {{SUSAN}—A New Approach to Low Level Image Processing},
	volume = {23},
	issn = {09205691},
	url = {http://link.springer.com/10.1023/A:1007963824710},
	doi = {10.1023/A:1007963824710},
	pages = {45--78},
	number = {1},
	journaltitle = {International Journal of Computer Vision},
	author = {Smith, Stephen M. and Brady, J. Michael},
	urldate = {2024-06-24},
	date = {1997},
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	rights = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	pages = {273--297},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	urldate = {2024-06-24},
	date = {1995-09},
	langid = {english},
	file = {Full Text:C\:\\Users\\gvisw\\Zotero\\storage\\HJL3NL52\\Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf},
}

@online{robtics_turtlebot3_nodate,
	title = {{TurtleBot}3 e-manual},
	url = {https://emanual.robotis.com/docs/en/platform/turtlebot3/overview/},
	author = {{ROBTICS}},
}

@online{intel_intel_nodate,
	title = {Intel Realsense D405 depth camera datasheet},
	url = {https://www.intelrealsense.com/depth-camera-d405/},
	author = {Intel},
}

@article{macenski_robot_2022,
	title = {Robot Operating System 2: Design, architecture, and uses in the wild},
	volume = {7},
	issn = {2470-9476},
	url = {https://www.science.org/doi/10.1126/scirobotics.abm6074},
	doi = {10.1126/scirobotics.abm6074},
	shorttitle = {Robot Operating System 2},
	abstract = {The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share—the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System ({ROS}) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, {ROS} 1 was not designed with many necessary production-grade features and algorithms. {ROS} 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of {ROS} 2 powering this new chapter in the robotics revolution. We also show through case studies the influence {ROS} 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.
          , 
            This Review describes {ROS} 2’s design, features, and performance with four case studies on land, air, sea, and even space.},
	pages = {eabm6074},
	number = {66},
	journaltitle = {Science Robotics},
	shortjournal = {Sci. Robot.},
	author = {Macenski, Steven and Foote, Tully and Gerkey, Brian and Lalancette, Chris and Woodall, William},
	urldate = {2024-06-24},
	date = {2022-05-25},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\LYS6AUQ9\\Macenski et al. - 2022 - Robot Operating System 2 Design, architecture, an.pdf:application/pdf},
}

@article{elfes_using_1989,
	title = {Using occupancy grids for mobile robot perception and navigation},
	volume = {22},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/30720/},
	doi = {10.1109/2.30720},
	pages = {46--57},
	number = {6},
	journaltitle = {Computer},
	shortjournal = {Computer},
	author = {Elfes, A.},
	urldate = {2024-06-24},
	date = {1989-06},
}

@incollection{vedaldi_end--end_2020,
	location = {Cham},
	title = {End-to-End Object Detection with Transformers},
	volume = {12346},
	isbn = {978-3-030-58451-1 978-3-030-58452-8},
	url = {https://link.springer.com/10.1007/978-3-030-58452-8_13},
	pages = {213--229},
	booktitle = {Computer Vision – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	urldate = {2024-06-24},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-58452-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\GM9RLFVE\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@article{herfandi_real-time_2024,
	title = {Real-Time Patient Indoor Health Monitoring and Location Tracking with Optical Camera Communications on the Internet of Medical Things},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/3/1153},
	doi = {10.3390/app14031153},
	abstract = {Optical Camera Communication ({OCC}) is an emerging technology that has attracted research interest in recent decades. Unlike previous communication technologies, {OCC} uses visible light as the medium to transmit data from receivers and cameras to receive the data. {OCC} has several advantages that can be capitalized in several implementations. However, the Internet of Things ({IoT}) has emerged as a technology with immense potential. Numerous research endeavors support the {IoT}’s prospective technology that can be implemented in various sectors, including the healthcare system. This study introduces a novel implementation of the Internet of Medical Things ({IoMT}) system, using {OCC} for real-time health monitoring and indoor location tracking. The innovative system uses standard closed-circuit television {CCTV} setups, integrating deep learning-based {OCC} to monitor multiple patients simultaneously, each represented by an {LED} matrix. The effectiveness of the system was demonstrated through two scenarios: the first involves dual transmitters and a single camera, highlighting real-time monitoring of vital health data; the second features a transmitter with dual cameras, focusing patient movement tracking across different camera fields of view. To accurately locate and track the position of {LED} arrays in the camera, the system used {YOLO} (You Only Look Once). Data are securely transmitted to an edge server and stored using the {REST} {API}, with a web interface providing real-time patient updates. This study highlights the potential of {OCC} in {IoMT} for advanced patient care and proposes future exploration in larger healthcare systems and other {IoT} domains.},
	pages = {1153},
	number = {3},
	journaltitle = {Applied Sciences},
	shortjournal = {Applied Sciences},
	author = {Herfandi, Herfandi and Sitanggang, Ones Sanjerico and Nasution, Muhammad Rangga Aziz and Nguyen, Huy and Jang, Yeong Min},
	urldate = {2024-06-25},
	date = {2024-01-30},
	langid = {english},
	file = {Full Text:C\:\\Users\\gvisw\\Zotero\\storage\\WVFY6E46\\Herfandi et al. - 2024 - Real-Time Patient Indoor Health Monitoring and Loc.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-06-27},
	date = {2023-08-01},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\gvisw\\Zotero\\storage\\LRLX49RY\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\gvisw\\Zotero\\storage\\CSB38Q3Z\\1706.html:text/html},
}

@misc{zhu_deformable_2020,
	title = {Deformable {DETR}: Deformable Transformers for End-to-End Object Detection},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2010.04159},
	doi = {10.48550/ARXIV.2010.04159},
	shorttitle = {Deformable {DETR}},
	abstract = {{DETR} has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable {DETR}, whose attention modules only attend to a small set of key sampling points around a reference. Deformable {DETR} can achieve better performance than {DETR} (especially on small objects) with 10 times less training epochs. Extensive experiments on the {COCO} benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-{DETR}.},
	publisher = {{arXiv}},
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	urldate = {2024-06-27},
	date = {2020},
	note = {Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@inproceedings{boser_training_1992,
	location = {Pittsburgh Pennsylvania {USA}},
	title = {A training algorithm for optimal margin classifiers},
	isbn = {978-0-89791-497-0},
	url = {https://dl.acm.org/doi/10.1145/130385.130401},
	doi = {10.1145/130385.130401},
	eventtitle = {{COLT}92: 5th Annual Workshop on Computational Learning Theory},
	pages = {144--152},
	booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
	publisher = {{ACM}},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	urldate = {2024-07-01},
	date = {1992-07},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\SVQ5AHIA\\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf:application/pdf},
}

@article{burges_tutorial_1998,
	title = {A Tutorial on Support Vector Machines for Pattern Recognition},
	volume = {2},
	issn = {13845810},
	url = {http://link.springer.com/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	pages = {121--167},
	number = {2},
	journaltitle = {Data Mining and Knowledge Discovery},
	author = {Burges, Christopher J.C.},
	urldate = {2024-07-01},
	date = {1998},
}

@book{cristianini_introduction_2000,
	edition = {1},
	title = {An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	rights = {https://www.cambridge.org/core/terms},
	isbn = {978-0-521-78019-3 978-0-511-80138-9},
	url = {https://www.cambridge.org/core/product/identifier/9780511801389/type/book},
	publisher = {Cambridge University Press},
	author = {Cristianini, Nello and Shawe-Taylor, John},
	urldate = {2024-07-01},
	date = {2000-03-23},
	doi = {10.1017/CBO9780511801389},
	file = {Submitted Version:C\:\\Users\\gvisw\\Zotero\\storage\\R26T5G26\\Cristianini and Shawe-Taylor - 2000 - An Introduction to Support Vector Machines and Oth.pdf:application/pdf},
}

@article{hsu_practical_2003,
	title = {A Practical Guide to Support Vector Classification Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin},
	author = {Hsu, Chih-wei and Chang, Chih-chung and Lin, Chih-Jen},
	date = {2003-11},
}

@incollection{carbonell_text_1998,
	location = {Berlin, Heidelberg},
	title = {Text categorization with Support Vector Machines: Learning with many relevant features},
	volume = {1398},
	isbn = {978-3-540-64417-0 978-3-540-69781-7},
	url = {http://link.springer.com/10.1007/BFb0026683},
	shorttitle = {Text categorization with Support Vector Machines},
	pages = {137--142},
	booktitle = {Machine Learning: {ECML}-98},
	publisher = {Springer Berlin Heidelberg},
	author = {Joachims, Thorsten},
	editor = {Nédellec, Claire and Rouveirol, Céline},
	editorb = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and Van Leeuwen, J.},
	editorbtype = {redactor},
	urldate = {2024-07-01},
	date = {1998},
	doi = {10.1007/BFb0026683},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Full Text:C\:\\Users\\gvisw\\Zotero\\storage\\ACCYJFKJ\\Joachims - 1998 - Text categorization with Support Vector Machines .pdf:application/pdf},
}

@book{scholkopf_learning_2001,
	title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
	isbn = {978-0-262-25693-3},
	url = {https://direct.mit.edu/books/book/1821/Learning-with-KernelsSupport-Vector-Machines},
	shorttitle = {Learning with Kernels},
	abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods.
            In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine ({SVM}). This gave rise to a new class of theoretically elegant learning machines that use a central concept of {SVMs}—-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.
            Learning with Kernels provides an introduction to {SVMs} and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
	publisher = {The {MIT} Press},
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	urldate = {2024-07-01},
	date = {2001-12-07},
	langid = {english},
	doi = {10.7551/mitpress/4175.001.0001},
}

@article{scholkopf_advances_1998,
	title = {Advances in Kernel Methods - Support Vector Learning},
	journaltitle = {{MIT} Press},
	author = {Scholkopf, Bernhard and Burges, Christopher and Smola, Alexander},
	date = {1998-12},
}

@book{noauthor_feature_2012,
	title = {Feature Extraction \& Image Processing for Computer Vision},
	isbn = {978-0-12-396549-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/C20110069351},
	publisher = {Elsevier},
	urldate = {2024-07-01},
	date = {2012},
	langid = {english},
	doi = {10.1016/C2011-0-06935-1},
}

@online{wikipedia_support_2024,
	title = {Support vector machine},
	url = {https://en.wikipedia.org/wiki/Support_vector_machine},
	author = {Wikipedia},
	date = {2024-06-25},
}
